{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random as rand\n",
    "\n",
    "# This code creates a virtual display to draw game images on. \n",
    "# If you are running locally, just ignore it\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Gym\n",
    "\n",
    "We're gonna spend several next weeks learning algorithms that solve decision processes. We are then in need of some interesting decision problems to test our algorithms.\n",
    "\n",
    "That's where OpenAI gym comes into play. It's a python library that wraps many classical decision problems including robot control, videogames and board games.\n",
    "\n",
    "So here's how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Observation space: Box(2,)\n",
      "Action space: Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "# plt.imshow(env.render('rgb_array'))\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if you're running this on your local machine, you'll see a window pop up with the image above. Don't close it, just alt-tab away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gym interface\n",
    "\n",
    "The three main methods of an environment are\n",
    "* __reset()__ - reset environment to initial state, _return first observation_\n",
    "* __render()__ - show current environment state (a more colorful version :) )\n",
    "* __step(a)__ - commit action __a__ and return (new observation, reward, is done, info)\n",
    " * _new observation_ - an observation right after commiting the action __a__\n",
    " * _reward_ - a number representing your reward for commiting action __a__\n",
    " * _is done_ - True if the MDP has just finished, False if still in progress\n",
    " * _info_ - some auxilary stuff about what just happened. Ignore it ~~for now~~."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial observation code: [-0.59081396  0.        ]\n"
     ]
    }
   ],
   "source": [
    "obs0 = env.reset()\n",
    "print(\"initial observation code:\", obs0)\n",
    "\n",
    "# Note: in MountainCar, observation is just two numbers: car position and velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taking action 2 (right)\n",
      "new observation code: [-0.58931326  0.0015007 ]\n",
      "reward: -1.0\n",
      "is game over?: False\n"
     ]
    }
   ],
   "source": [
    "print(\"taking action 2 (right)\")\n",
    "new_obs, reward, is_done, _ = env.step(2)\n",
    "\n",
    "print(\"new observation code:\", new_obs)\n",
    "print(\"reward:\", reward)\n",
    "print(\"is game over?:\", is_done)\n",
    "\n",
    "# Note: as you can see, the car has moved to the riht slightly (around 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.54068619,  0.00113637]), -1.0, False, {})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with it\n",
    "\n",
    "Below is the code that drives the car to the right. \n",
    "\n",
    "However, it doesn't reach the flag at the far right due to gravity. \n",
    "\n",
    "__Your task__ is to fix it. Find a strategy that reaches the flag. \n",
    "\n",
    "You're not required to build any sophisticated algorithms for now, feel free to hard-code :)\n",
    "\n",
    "_Hint: your action at each step should depend either on __t__ or on __s__._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create env manually to set time limit. Please don't change this.\n",
    "TIME_LIMIT = 250\n",
    "env = gym.wrappers.TimeLimit(gym.envs.classic_control.MountainCarEnv(),\n",
    "                             max_episode_steps=TIME_LIMIT + 1)\n",
    "s = env.reset()\n",
    "actions = {'left': 0, 'stop': 1, 'right': 2}\n",
    "\n",
    "# prepare \"display\"\n",
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "fig.show()\n",
    "\n",
    "def policy(t):\n",
    "    if t<50:\n",
    "        return actions['left']\n",
    "    if t<100:\n",
    "        return actions['right']\n",
    "    if t<150:\n",
    "        return actions['left']\n",
    "    return actions['right']\n",
    "\n",
    "\n",
    "for t in range(TIME_LIMIT):\n",
    "    \n",
    "    s, r, done, _ = env.step(policy(t))\n",
    "    \n",
    "    #draw game image on display\n",
    "    ax.clear()\n",
    "    ax.imshow(env.render('rgb_array'))\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    if done:\n",
    "        print(\"Well done!\")\n",
    "        break\n",
    "else:    \n",
    "    print(\"Time limit exceeded. Try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_policy(t):\n",
    "    if t<50:\n",
    "        return actions['left']\n",
    "    if t<100:\n",
    "        return actions['right']\n",
    "    if t<150:\n",
    "        return actions['left']\n",
    "    return actions['right']\n",
    "\n",
    "def random_policy(t):\n",
    "    choice = rand.choice(['left','right'])\n",
    "    return actions[choice]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def game(env, policy):\n",
    "    env.reset()\n",
    "    for t in range(TIME_LIMIT):\n",
    "        s, r, done, _ = env.step(policy(t))\n",
    "        if done:\n",
    "            return True\n",
    "    return False\n",
    "        \n",
    "env = gym.wrappers.TimeLimit(gym.envs.classic_control.MountainCarEnv(), max_episode_steps=TIME_LIMIT + 1)\n",
    "\n",
    "game(env, random_policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def run_cycle(cycle_len = 1000):\n",
    "    env = gym.wrappers.TimeLimit(gym.envs.classic_control.MountainCarEnv(), max_episode_steps=TIME_LIMIT + 1)\n",
    "    for i in range(cycle_len):\n",
    "        if game(env, random_policy):\n",
    "            print('Finished')\n",
    "            \n",
    "run_cycle()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to coursera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from submit import submit_interface\n",
    "submit_interface(policy, <EMAIL>, <TOKEN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61527417, 0.085821  , 1.62472966])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "solution = np.array([125, 0.1, -0.3])\n",
    "def f(w): return -np.sum((w - solution)**2)\n",
    "\n",
    "npop = 100      # population size\n",
    "sigma = 0.1    # noise standard deviation\n",
    "alpha = 0.001  # learning rate\n",
    "w = np.random.randn(3) # initial guess\n",
    "for i in range(30):\n",
    "    N = np.random.randn(npop, 3)\n",
    "    R = np.zeros(npop)\n",
    "    for j in range(npop):\n",
    "        w_try = w + sigma*N[j]\n",
    "        R[j] = f(w_try)\n",
    "    A = (R - np.mean(R)) / np.std(R)\n",
    "    w = w + alpha/(npop*sigma) * np.dot(N.T, A)\n",
    "\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-5.05791681, -2.76025832,  1.24975011,  0.71110893],\n",
       "       [-5.05791681, -2.76025832,  1.24975011,  0.71110893],\n",
       "       [-5.05791681, -2.76025832,  1.24975011,  0.71110893],\n",
       "       [-5.05791681, -2.76025832,  1.24975011,  0.71110893]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = np.ones((3,4))\n",
    "m2 = np.random.randn(3,4)\n",
    "print(m2.shape)\n",
    "np.dot(m1.T,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "??np.dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 rl",
   "language": "python",
   "name": "python3_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
