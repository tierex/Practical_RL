{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "env_name='FrozenLake8x8-v0'\n",
    "env = gym.make(env_name)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 8, 0.0, False)],\n",
       " 1: [(0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 8, 0.0, False),\n",
       "  (0.3333333333333333, 1, 0.0, False)],\n",
       " 2: [(0.3333333333333333, 8, 0.0, False),\n",
       "  (0.3333333333333333, 1, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False)],\n",
       " 3: [(0.3333333333333333, 1, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False),\n",
       "  (0.3333333333333333, 0, 0.0, False)]}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.P[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333 63 1.0\n",
      "0.3333333333333333 63 1.0\n",
      "0.3333333333333333 63 1.0\n",
      "0.3333333333333333 63 1.0\n",
      "0.3333333333333333 63 1.0\n",
      "0.3333333333333333 63 1.0\n"
     ]
    }
   ],
   "source": [
    "P = env.env.P\n",
    "a = P[0]\n",
    "for s in P:\n",
    "    for a in P[s]:\n",
    "        for p,s,r,_ in P[s][a]:\n",
    "            if r!=0:\n",
    "                print(p,s,r)\n",
    "# [P[0][x] for x in P[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after  2357\n",
      "[1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.97820163\n",
      " 0.92643052 0.         0.85661768 0.94623163 0.98207721 1.\n",
      " 1.         0.9346049  0.80108992 0.47490377 0.6236214  0.\n",
      " 0.94467761 1.         1.         0.82561308 0.54223433 0.\n",
      " 0.53934275 0.61118923 0.85195561 1.         1.         0.\n",
      " 0.         0.16804079 0.38321763 0.44226934 0.         1.\n",
      " 1.         0.         0.19467347 0.12090475 0.         0.33240114\n",
      " 0.         1.         1.         0.73155782 0.46311564 0.\n",
      " 0.27746705 0.5549341  0.77746705 0.        ]\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(env, gamma=1.0):\n",
    "    eps = 1e-20\n",
    "    v = np.zeros(env.nS)\n",
    "    iters = 10000\n",
    "    for i in range(iters):\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            qa = np.zeros(env.nA)\n",
    "            for a in range(env.nA):\n",
    "                qa[a] = sum([p*(r+gamma*prev_v[s_]) for p,s_,r,_ in env.P[s][a]])\n",
    "            v[s] = max(qa)\n",
    "        if np.sum(np.abs(prev_v-v)) <=eps:\n",
    "            print('Converged after ', i+1)\n",
    "            break\n",
    "    return v\n",
    "\n",
    "optimal_v = value_iteration(env.env, gamma=1.0)\n",
    "print(optimal_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 2. 1. 2. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 2. 0. 0. 0. 0. 2. 3. 3. 2.\n",
      " 0. 0. 0. 1. 0. 0. 2. 2. 0. 3. 0. 0. 2. 1. 3. 2. 0. 0. 0. 1. 3. 0. 0. 2.\n",
      " 0. 0. 1. 0. 0. 0. 0. 2. 0. 1. 0. 0. 1. 2. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_policy(env,v, gamma = 1.0):\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        q_sa = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for p,s_,r,_ in P[s][a]:\n",
    "                q_sa[a]+=p*(r+gamma*v[s_])\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "policy = compute_policy(env.env, optimal_v, gamma =1.0)\n",
    "\n",
    "print(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.318269218083098e-07"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs,reward, done, _ = env.step(int(policy[obs]))\n",
    "        total_reward += gamma ** step_idx * reward\n",
    "        step_idx +=1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "run_episode(env, policy, gamma =0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.883, 0.883)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0, n = 100):\n",
    "    scores = np.array([run_episode(env, policy, gamma) for _ in range(n)])\n",
    "    finished_games = scores[scores != 0.0]\n",
    "    return np.mean(scores), len(finished_games)/len(scores)\n",
    "\n",
    "evaluate_policy(env,policy,n=1000, gamma = 1.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_value_function(env, policy, gamma = 1.0):\n",
    "    iters = 10000\n",
    "    v = np.zeros(env.nS)\n",
    "    eps = 1e-15\n",
    "    for i in range(0, iters):\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(0,env.nS):\n",
    "            policy_action = policy[s]\n",
    "            v[s] = sum([p*(r+gamma*prev_v[s_]) for p,s_,r,_ in env.P[s][policy_action]])\n",
    "        if np.sum(np.fabs(prev_v - v)) <=eps:\n",
    "            break\n",
    "    return v\n",
    "\n",
    "def extract_policy(env, v, gamma = 1.0):\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(0,env.nS):\n",
    "        q_sa = np.zeros(env.nA)\n",
    "        for a in range(0,env.nA):\n",
    "            q_sa[a] = sum([p*(r+gamma*v[s_]) for p,s_,r,_ in env.P[s][a]])\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "def policy_iteration(env, gamma = 1.0):\n",
    "    policy = np.random.choice(env.nA, size = env.nS)\n",
    "    iters = 10000\n",
    "    for i in range(0,iters):\n",
    "        old_policy_v = compute_value_function(env, policy, gamma)\n",
    "        new_policy = extract_policy(env, old_policy_v, gamma)\n",
    "        if np.all(new_policy == policy):\n",
    "            print('Policy iteration converged after ', i)\n",
    "            break;\n",
    "        policy = new_policy\n",
    "    return policy\n",
    "\n",
    "\n",
    "opt_policy = policy_iteration(env.env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 2., 2., 2., 2., 2., 2., 2., 3., 3., 3., 3., 3., 3., 3., 2., 0.,\n",
       "       0., 0., 0., 2., 3., 3., 2., 0., 0., 0., 1., 0., 0., 2., 2., 0., 3.,\n",
       "       0., 0., 2., 1., 3., 2., 0., 0., 0., 1., 3., 0., 0., 2., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 2., 0., 1., 0., 0., 1., 2., 1., 0.])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.88, 0.88)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(env,opt_policy,n=1000, gamma = 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 rl",
   "language": "python",
   "name": "python3_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
